{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6988c652-3ce5-45ee-acce-84c32ee6cb8d",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55534705-0fd9-4bfb-a7bf-488ad5370370",
   "metadata": {},
   "source": [
    "Ans - Min-Max scaling, also known as normalization, is a data preprocessing technique used in machine learning to transform the features of a dataset to a specific range, typically between 0 and 1. This scaling method is useful when you have features with different scales or units, as it helps ensure that all features contribute equally to the model's training process.\n",
    "\n",
    "The formula for Min-Max scaling is as follows for a single feature:\n",
    "\\[X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{\\text{scaled}}\\) is the scaled value of the feature.\n",
    "- \\(X\\) is the original value of the feature.\n",
    "- \\(X_{\\text{min}}\\) is the minimum value of the feature in the dataset.\n",
    "- \\(X_{\\text{max}}\\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset with a single feature, \"Age,\" which ranges from 20 to 60 years. The goal is to scale this feature to a range between 0 and 1 using Min-Max scaling.\n",
    "\n",
    "1. Find the minimum and maximum values of the \"Age\" feature in the dataset:\n",
    "   - \\(X_{\\text{min}} = 20\\) (minimum age)\n",
    "   - \\(X_{\\text{max}} = 60\\) (maximum age)\n",
    "\n",
    "2. Choose a data point from the dataset, let's say \\(X = 30\\) (representing a person with an age of 30 years).\n",
    "\n",
    "3. Apply the Min-Max scaling formula:\n",
    "   \\[X_{\\text{scaled}} = \\frac{30 - 20}{60 - 20} = \\frac{10}{40} = 0.25\\]\n",
    "\n",
    "So, the scaled value for an age of 30 years is 0.25 after Min-Max scaling. You can perform the same scaling operation for all the data points in the \"Age\" feature, ensuring that they are all transformed to the range [0, 1].\n",
    "\n",
    "Min-Max scaling is particularly useful when you're working with machine learning algorithms that are sensitive to the scale of input features, such as support vector machines (SVMs) and k-means clustering. It helps prevent features with larger ranges from dominating the learning process and makes it easier for the model to converge effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b6b3b-543d-4621-8711-b249b96b3c6b",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94fbd6f-f949-478c-a092-9107e24fc2c2",
   "metadata": {},
   "source": [
    "Ans - The Unit Vector technique, also known as \"Normalization\" in some contexts, is a feature scaling method used in data preprocessing in machine learning. Unlike Min-Max scaling, which scales features to a specific range (typically between 0 and 1), the Unit Vector technique scales features to have a magnitude of 1 while preserving their direction. This is achieved by dividing each data point by the magnitude (Euclidean norm) of the feature vector.\n",
    "\n",
    "The formula for the Unit Vector technique for a single feature is as follows:\n",
    "\\[X_{\\text{unit}} = \\frac{X}{\\|X\\|}\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{\\text{unit}}\\) is the unit-scaled value of the feature.\n",
    "- \\(X\\) is the original value of the feature.\n",
    "- \\(\\|X\\|\\) is the magnitude of the feature vector.\n",
    "\n",
    "Here's an example to illustrate the Unit Vector technique:\n",
    "\n",
    "Suppose you have a dataset with two features, \"Height\" and \"Weight,\" and you want to scale these features using the Unit Vector technique to normalize them.\n",
    "\n",
    "1. Calculate the magnitude (\\(\\|X\\|\\)) of the feature vector for each data point. The magnitude for a 2D vector is calculated as follows:\n",
    "   \\[\\|X\\| = \\sqrt{\\text{Height}^2 + \\text{Weight}^2}\\]\n",
    "\n",
    "2. Choose a data point with \"Height\" = 160 cm and \"Weight\" = 70 kg. Calculate its magnitude:\n",
    "   \\[\\|X\\| = \\sqrt{160^2 + 70^2} = \\sqrt{25600 + 4900} = \\sqrt{30500} \\approx 174.89\\]\n",
    "\n",
    "3. Apply the Unit Vector technique to normalize the data point:\n",
    "   \\[\\text{Height}_{\\text{unit}} = \\frac{160}{174.89} \\approx 0.914\\]\n",
    "   \\[\\text{Weight}_{\\text{unit}} = \\frac{70}{174.89} \\approx 0.399\\]\n",
    "\n",
    "So, the \"Height\" and \"Weight\" values are scaled such that the magnitude of the resulting vector is approximately 1. These unit-scaled values retain the direction of the original data but ensure that all feature vectors have the same magnitude.\n",
    "\n",
    "Differences between Min-Max Scaling and Unit Vector (Normalization):\n",
    "- Min-Max Scaling scales features to a specific range (e.g., [0, 1]), while Unit Vector scaling scales features to have a magnitude of 1.\n",
    "- Min-Max Scaling preserves the relative differences between data points but may not preserve the direction of the original data. Unit Vector scaling preserves both the relative differences and the direction of the data.\n",
    "- Min-Max Scaling is suitable for algorithms that assume features are within a certain range, while Unit Vector scaling is suitable for algorithms that don't make strong assumptions about feature scales and where the direction of the data matters (e.g., Principal Component Analysis, clustering algorithms).\n",
    "- Min-Max Scaling may not work well if the data contains outliers, whereas Unit Vector scaling is less affected by outliers as it focuses on the direction of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb588034-6091-47c1-88ce-c42c07685bc7",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c1813-34d3-4340-a636-66fa87d9afb9",
   "metadata": {},
   "source": [
    "Ans - Principal Component Analysis (PCA) is a dimensionality reduction technique used in data analysis and machine learning. Its primary goal is to reduce the dimensionality of a dataset while preserving as much of the original variability or information as possible. PCA accomplishes this by transforming the original features into a new set of orthogonal (uncorrelated) features called principal components.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "1. **Standardization**: PCA begins by standardizing the data (mean centering and scaling to unit variance) to ensure that all features have the same scale. Standardization is essential because PCA is sensitive to the scale of the features.\n",
    "\n",
    "2. **Covariance Matrix**: PCA calculates the covariance matrix of the standardized data. The covariance matrix captures the relationships between features, indicating how they vary together.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: PCA then performs an eigenvalue decomposition (or singular value decomposition) on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "4. **Selecting Principal Components**: The eigenvalues represent the variance explained by each corresponding eigenvector (principal component). PCA sorts the eigenvalues in descending order. By selecting a subset of these principal components, you can retain a significant portion of the dataset's variance while reducing dimensionality.\n",
    "\n",
    "5. **Transformation**: The selected principal components are used to transform the original data into a new feature space. Each data point is projected onto the principal components, creating a reduced-dimensional representation of the data.\n",
    "\n",
    "PCA is widely used for various purposes, including dimensionality reduction, noise reduction, and data visualization. It's particularly useful when dealing with high-dimensional data, such as in image processing or genomics.\n",
    "\n",
    "Here's an example to illustrate PCA's application in dimensionality reduction:\n",
    "\n",
    "Suppose you have a dataset of face images, each represented by pixel values. Each image has 1,000 pixel features, making it challenging to work with due to the high dimensionality. You want to reduce the dimensionality of the dataset while preserving most of the important facial features.\n",
    "\n",
    "1. **Standardization**: Standardize the pixel values so that each feature has zero mean and unit variance.\n",
    "\n",
    "2. **Covariance Matrix**: Calculate the covariance matrix of the standardized data. This matrix captures how pixel values correlate with each other across the images.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix. This yields a set of eigenvalues and corresponding eigenvectors.\n",
    "\n",
    "4. **Selecting Principal Components**: Sort the eigenvalues in descending order and choose the top \\(k\\) eigenvectors corresponding to the largest eigenvalues. These eigenvectors represent the principal components.\n",
    "\n",
    "5. **Transformation**: Project the original face images onto the selected principal components. This reduces each image's dimensionality from 1,000 pixels to \\(k\\) dimensions.\n",
    "\n",
    "By choosing an appropriate value of \\(k\\), you can achieve a trade-off between reducing dimensionality and preserving most of the original information. Smaller values of \\(k\\) result in greater dimensionality reduction but may sacrifice some detail in the images. PCA allows you to make this choice based on the variance explained by the selected principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a784c9ac-a694-4fe5-85ed-0a545c34bfd6",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6911689-d03d-4d6b-b93e-b7e428be7228",
   "metadata": {},
   "source": [
    "Ans - PCA (Principal Component Analysis) and feature extraction are closely related concepts in machine learning and data analysis. PCA can be used as a feature extraction technique to reduce the dimensionality of a dataset while retaining its most important information. Here's how PCA relates to feature extraction and how it can be used for this purpose:\n",
    "\n",
    "1. **Dimensionality Reduction**: Both PCA and feature extraction aim to reduce the dimensionality of a dataset. High-dimensional datasets can suffer from the curse of dimensionality, which can lead to increased computational complexity, overfitting, and difficulty in visualization and interpretation.\n",
    "\n",
    "2. **PCA as Feature Extraction**: PCA acts as a feature extraction method by transforming the original features into a new set of features called principal components. These principal components are linear combinations of the original features and are chosen to maximize the variance in the data.\n",
    "\n",
    "3. **Retaining Information**: PCA selects principal components in a way that retains as much of the original variability or information as possible. The first principal component captures the most variance, the second captures the second most, and so on. By selecting a subset of these components, you can effectively reduce the dimensionality of the data while preserving its essential characteristics.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose you have a dataset of handwritten digits, each represented by a 28x28-pixel image. This results in a high-dimensional feature space with 784 features (28 * 28). You want to perform digit recognition but need to reduce the dimensionality of the data for efficient modeling.\n",
    "\n",
    "1. **Data Preparation**: Standardize the pixel values of the images to have zero mean and unit variance.\n",
    "\n",
    "2. **PCA Transformation**: Apply PCA to the standardized data. PCA will identify the principal components that capture the most significant variations among the images.\n",
    "\n",
    "3. **Selecting Components**: Examine the explained variance ratio associated with each principal component. You might find that the first 50 principal components capture, for example, 95% of the total variance in the data. You can choose to keep these 50 components.\n",
    "\n",
    "4. **Feature Extraction**: Project the original images onto the selected 50 principal components. Each image is now represented as a 50-dimensional vector, which serves as a feature vector. These 50 features are a compressed representation of the original 784-pixel features.\n",
    "\n",
    "5. **Classification**: Train a machine learning model (e.g., a classifier like a support vector machine or a neural network) using the reduced-dimensional feature vectors for digit recognition. The reduced dimensionality speeds up training and may even improve model performance by reducing overfitting.\n",
    "\n",
    "In this example, PCA has been used for feature extraction, effectively reducing the dimensionality of the dataset from 784 features to 50 features while retaining most of the essential information for digit recognition. This makes the dataset more manageable and can lead to more efficient and accurate machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7af992-8faf-4250-a807-8b536216b5fa",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e401da-8850-44c5-9425-3f7cfa27771b",
   "metadata": {},
   "source": [
    "Ans - Min-Max scaling is a common preprocessing technique used to transform features in a dataset to a specific range, typically between 0 and 1. In the context of building a recommendation system for a food delivery service with features like price, rating, and delivery time, you can use Min-Max scaling as follows:\n",
    "\n",
    "1. **Understand the Features**: First, you should have a good understanding of the features in your dataset. In your case, you mentioned three features: price, rating, and delivery time. It's essential to know the range and distribution of each feature.\n",
    "\n",
    "2. **Standardization or Normalization**: While Min-Max scaling is a form of normalization, you might also consider standardizing the data (mean centering and scaling to unit variance) if your features have different scales. Standardization is crucial when working with algorithms that are sensitive to feature scales, such as k-means clustering or gradient-based optimization methods like gradient descent for neural networks.\n",
    "\n",
    "3. **Min-Max Scaling**: To perform Min-Max scaling on your features, you'll follow these steps for each feature:\n",
    "\n",
    "   - Determine the minimum (\\(X_{\\text{min}}\\)) and maximum (\\(X_{\\text{max}}\\)) values of the feature in your dataset.\n",
    "   \n",
    "   - Apply the Min-Max scaling formula for each data point in the feature:\n",
    "   \n",
    "     \\[X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\]\n",
    "\n",
    "   - Replace the original values of the feature with the scaled values.\n",
    "\n",
    "4. **Range Transformation**: After applying Min-Max scaling, the values of each feature will be transformed to the range [0, 1]. This transformation ensures that all the features have the same scale, which can be crucial for some recommendation algorithms.\n",
    "\n",
    "Here's how Min-Max scaling would be applied to your specific features:\n",
    "\n",
    "- **Price**: If the price ranges from, for example, $5 to $25, after Min-Max scaling, it will be transformed to a range of [0, 1], where $5 maps to 0, and $25 maps to 1.\n",
    "\n",
    "- **Rating**: If the rating ranges from 2 to 5 (with 5 being the highest), after Min-Max scaling, it will be transformed to a range of [0, 1], where 2 maps to 0, and 5 maps to 1.\n",
    "\n",
    "- **Delivery Time**: If the delivery time ranges from 10 minutes to 60 minutes, after Min-Max scaling, it will be transformed to a range of [0, 1], where 10 minutes maps to 0, and 60 minutes maps to 1.\n",
    "\n",
    "By performing Min-Max scaling, you ensure that these features are on a consistent scale, which can be important when building a recommendation system. It helps prevent features with larger numerical ranges from having a disproportionately large influence on the recommendation process, and it can improve the performance of recommendation algorithms that rely on feature similarity or distance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcafc30-d69b-47ab-9678-81949e0024a0",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d3316-2181-4374-991e-858897fbff88",
   "metadata": {},
   "source": [
    "Ans - Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset containing many features for predicting stock prices can be a valuable preprocessing step. By reducing the number of features while retaining most of the information, you can simplify the modeling process, potentially reduce overfitting, and improve the model's efficiency. Here's how you can use PCA in this context:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - **Standardization**: Start by standardizing the features in your dataset. This involves subtracting the mean and dividing by the standard deviation for each feature. Standardization is crucial because PCA is sensitive to the scale of the features, and it ensures that all features have a similar scale.\n",
    "\n",
    "2. **PCA Transformation**:\n",
    "   - **Covariance Matrix**: Calculate the covariance matrix of the standardized data. The covariance matrix captures the relationships and variations between different features.\n",
    "   \n",
    "   - **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvalues and eigenvectors. These eigenvectors represent the principal components of the data.\n",
    "   \n",
    "   - **Sort Eigenvalues**: Sort the eigenvalues in descending order. The eigenvalues indicate how much variance is explained by each corresponding eigenvector. Higher eigenvalues correspond to principal components that capture more of the dataset's variance.\n",
    "\n",
    "   - **Select Principal Components**: Decide on the number of principal components (features) you want to retain in your reduced-dimensional dataset. You can choose a number based on the cumulative explained variance. For instance, you might aim to retain 90% or 95% of the total variance.\n",
    "\n",
    "   - **Projection**: Project the original data onto the selected principal components to obtain a reduced-dimensional dataset. Each data point is represented by a linear combination of these components.\n",
    "\n",
    "3. **Modeling**:\n",
    "   - Train your stock price prediction model using the reduced-dimensional dataset obtained after PCA.\n",
    "\n",
    "Here are some considerations for using PCA for stock price prediction:\n",
    "\n",
    "- **Interpretability**: Keep in mind that after PCA, the features in your reduced-dimensional dataset are linear combinations of the original features. While this can improve modeling efficiency, it may make the interpretability of your model more challenging. You'll need to map predictions back to the original features if you want to understand the factors driving your model's decisions.\n",
    "\n",
    "- **Feature Engineering**: PCA doesn't consider the specific meaning of features, which may not always be ideal for stock price prediction. It's often valuable to combine PCA with domain-specific feature engineering to capture relevant financial indicators, market trends, and sentiment data.\n",
    "\n",
    "- **Model Selection**: Choose an appropriate machine learning algorithm for stock price prediction that works well with high-dimensional data. Algorithms like support vector machines, random forests, or deep learning models (e.g., recurrent neural networks) are commonly used for such tasks.\n",
    "\n",
    "- **Hyperparameter Tuning**: Depending on the number of principal components you choose, you may need to reevaluate hyperparameters for your chosen machine learning algorithm.\n",
    "\n",
    "- **Evaluation**: Evaluate your model's performance using appropriate metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), or other relevant measures. Be sure to compare the performance of models with and without PCA to determine if dimensionality reduction is beneficial for your specific dataset.\n",
    "\n",
    "By applying PCA as part of your data preprocessing pipeline, you can potentially reduce noise in the data, improve model efficiency, and focus on the most relevant information for stock price prediction. However, it's essential to strike a balance between dimensionality reduction and information retention to ensure your model's predictive power is not compromised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc65b85-ec7a-4f35-bec7-915c78462d48",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132d7ca-e1d8-432f-8a0f-70a06295b681",
   "metadata": {},
   "source": [
    "Ans - To perform Min-Max scaling on a dataset and transform the values to a range of -1 to 1, you can follow these steps:\n",
    "\n",
    "1. Determine the minimum and maximum values in the original dataset.\n",
    "2. Apply the Min-Max scaling formula to each data point in the dataset.\n",
    "3. The scaled values will fall within the specified range (-1 to 1).\n",
    "\n",
    "Here's how you can do it for the dataset: [1, 5, 10, 15, 20]:\n",
    "\n",
    "Step 1: Find the minimum and maximum values in the dataset.\n",
    "- Minimum value (\\(X_{\\text{min}}\\)) = 1\n",
    "- Maximum value (\\(X_{\\text{max}}\\)) = 20\n",
    "\n",
    "Step 2: Apply the Min-Max scaling formula to each data point:\n",
    "- For the value 1:\n",
    "  \\[X_{\\text{scaled}} = \\frac{1 - 1}{20 - 1} = 0\\]\n",
    "\n",
    "- For the value 5:\n",
    "  \\[X_{\\text{scaled}} = \\frac{5 - 1}{20 - 1} = \\frac{4}{19}\\]\n",
    "\n",
    "- For the value 10:\n",
    "  \\[X_{\\text{scaled}} = \\frac{10 - 1}{20 - 1} = \\frac{9}{19}\\]\n",
    "\n",
    "- For the value 15:\n",
    "  \\[X_{\\text{scaled}} = \\frac{15 - 1}{20 - 1} = \\frac{14}{19}\\]\n",
    "\n",
    "- For the value 20:\n",
    "  \\[X_{\\text{scaled}} = \\frac{20 - 1}{20 - 1} = 1\\]\n",
    "\n",
    "Now, your dataset, after Min-Max scaling, will look like this:\n",
    "\n",
    "\\[[-1, \\frac{4}{19}, \\frac{9}{19}, \\frac{14}{19}, 1]\\]\n",
    "\n",
    "All values are within the specified range of -1 to 1, with 1 representing the maximum value in the original dataset, and -1 representing the minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd8f5ce-fa21-43de-a2f1-3bedcda2668b",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d06512-7bed-4a3a-826b-15ea6ec96175",
   "metadata": {},
   "source": [
    "Ans - Deciding how many principal components to retain in a PCA-based feature extraction process depends on your specific goals, the characteristics of the dataset, and the trade-off between dimensionality reduction and information preservation. Here are some steps you can follow to determine the number of principal components to retain:\n",
    "\n",
    "1. **Standardization**: Start by standardizing your features (height, weight, age, and blood pressure) to have zero mean and unit variance. This ensures that all features are on the same scale, which is essential for PCA.\n",
    "\n",
    "2. **Covariance Matrix**: Calculate the covariance matrix of the standardized data. The covariance matrix summarizes how features vary together.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: Perform eigenvalue decomposition on the covariance matrix. This will yield a set of eigenvalues and corresponding eigenvectors.\n",
    "\n",
    "4. **Sort Eigenvalues**: Sort the eigenvalues in descending order. Each eigenvalue represents the amount of variance explained by its corresponding eigenvector (principal component).\n",
    "\n",
    "5. **Cumulative Explained Variance**: Calculate the cumulative explained variance as you consider an increasing number of principal components. The cumulative explained variance tells you how much of the total variance in the dataset is retained by including a certain number of principal components.\n",
    "\n",
    "6. **Choose the Number of Principal Components**: Decide on the number of principal components to retain based on your desired level of explained variance. Common thresholds include retaining enough components to explain 90%, 95%, or 99% of the total variance.\n",
    "\n",
    "The choice of the number of principal components depends on your specific use case:\n",
    "\n",
    "- **High Information Retention**: If preserving most of the information is critical and computational resources are not a major concern, you may choose to retain enough principal components to explain 95% or 99% of the total variance. This ensures that you retain as much of the original information as possible.\n",
    "\n",
    "- **Balanced Trade-off**: If you want to strike a balance between dimensionality reduction and information retention, you might aim to retain enough components to explain 90% of the total variance. This reduces the dimensionality while still retaining a significant portion of the data's variation.\n",
    "\n",
    "- **Dimensionality Reduction**: If computational efficiency is a primary concern, you might choose to retain a smaller number of principal components, perhaps those explaining 80% or 85% of the total variance. This sacrifices some information for the sake of reducing dimensionality.\n",
    "\n",
    "It's important to note that the choice of the number of principal components should be guided by domain knowledge and the specific requirements of your prediction or analysis task. After deciding on the number of components to retain, you can project your data onto these components to create the reduced-dimensional feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64f8fdc-2ff3-44dd-b3a6-87162fc6c37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
